UNet trainer for SDXL models that requires ~12 GB of VRAM. A bit lower if only using LoRA Mode.    

Modified from [Aozora](https://github.com/Hysocs/Aozora_SDXL_Training) for personal usage.    

Features:    

- Precomputed latent caching.
- Resume state from a training checkpoint.    
- UNet layer exclusion via keyword filter.    
- LoRA Mode (minimal toggle that freezes layers).
- Tag caption dropout with wildcards.    
- More timestep sampling choices.
- Flow matching with FlowMatchEulerDiscrete scheduler.  
- Optimizer buffers stored in fp32.  
- Optimizer state offload frequency.
- Optional reflection padding for EQ-VAE.
- Improved GUI with neutral blue theme.
- Headless mode via cli wrapper.     
- Bonus utility scripts.    

Usage:

`git clone https://github.com/geltz/sdxl-trainer`  
`python -m venv venv` or use uv: `uv venv venv`    
`./venv/scripts/activate`    
`pip install -r requirements.txt` or use uv here    
`python gui.py`    

If you want to run headless:  
`python cli.py configname`

This runs a json generated by `gui.py` by wrapping into `train.py`. 

Example configs are provided. Trainer will automatically generate a default one if there's none.  
